{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944069e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import cdsapi\n",
    "from datetime import datetime, timedelta\n",
    "from mpi4py import MPI\n",
    "import os\n",
    "\n",
    "def get_years_and_months(start_date, end_date):\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    years = set()\n",
    "    months = set()\n",
    "    \n",
    "    current = start.replace(day=1)\n",
    "    \n",
    "    while current <= end:\n",
    "        years.add(str(current.year))\n",
    "        months.add(f\"{current.month:02d}\")\n",
    " \n",
    "        if current.month == 12:\n",
    "            current = current.replace(year=current.year + 1, month=1)\n",
    "        else:\n",
    "            current = current.replace(month=current.month + 1)\n",
    "    \n",
    "    return sorted(years), sorted(months)\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()  # Process ID\n",
    "size = comm.Get_size()  # Number of processes\n",
    "\n",
    "# Ensure 'data/' directory exists\n",
    "DATA_DIR = \"data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "def get_monthly_chunks(start_date, end_date):\n",
    "    \"\"\"Splits the date range into 1-month chunks.\"\"\"\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "\n",
    "    chunks = []\n",
    "    current_start = start.replace(day=1)\n",
    "\n",
    "    while current_start <= end:\n",
    "        next_end = (current_start + timedelta(days=32)).replace(day=1) - timedelta(days=1)  # Last day of the month\n",
    "        if next_end > end:\n",
    "            next_end = end\n",
    "        \n",
    "        chunks.append((current_start.strftime(\"%Y-%m-%d\"), next_end.strftime(\"%Y-%m-%d\")))\n",
    "        current_start = next_end + timedelta(days=1)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def download_era5_data(dataset, start, end, area, variables, pressure_levels=None):\n",
    "    \"\"\"Downloads ERA5 data only if it doesn't already exist.\"\"\"\n",
    "    name = os.path.join(DATA_DIR, f\"{dataset}-{start}-{end}.nc\")\n",
    "\n",
    "    # Check if file already exists\n",
    "    if os.path.exists(name) and os.path.getsize(name) > 0:\n",
    "        print(f\"Skipping already downloaded file: {name}\")\n",
    "        return name\n",
    "\n",
    "    year = start[:4]\n",
    "    month = start[5:7]\n",
    "\n",
    "    request = {\n",
    "        \"product_type\": dataset,\n",
    "        \"variable\": variables,\n",
    "        \"year\": [year],\n",
    "        \"month\": [month],\n",
    "        \"day\": [f\"{day:02d}\" for day in range(1, 32)],\n",
    "        \"time\": [f\"{hour:02d}:00\" for hour in range(24)],\n",
    "        \"data_format\": \"netcdf\",\n",
    "        \"area\": area\n",
    "    }\n",
    "\n",
    "    if pressure_levels:\n",
    "        request[\"pressure_level\"] = pressure_levels\n",
    "\n",
    "    client = cdsapi.Client()\n",
    "    client.retrieve(dataset, request, name)\n",
    "    \n",
    "    print(f\"Downloaded: {name}\")\n",
    "    return name\n",
    "\n",
    "def merge_netcdf_files(files, output_name):\n",
    "    \"\"\"Merges multiple NetCDF files into one dataset.\"\"\"\n",
    "    output_path = os.path.join(DATA_DIR, output_name)\n",
    "\n",
    "    if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
    "        print(f\"Merged file already exists: {output_path}\")\n",
    "        return output_path\n",
    "\n",
    "    datasets = [xr.open_dataset(f) for f in files]\n",
    "    merged_ds = xr.concat(datasets, dim=\"time\")\n",
    "    merged_ds.to_netcdf(output_path)\n",
    "\n",
    "    print(f\"Merged NetCDF saved as: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "def parallel_download_era5(dataset, start, end, area, variables, pressure_levels=None):\n",
    "    \"\"\"Uses MPI to download ERA5 data in parallel by distributing monthly chunks across processes.\"\"\"\n",
    "    chunks = get_monthly_chunks(start, end)\n",
    "    files = []\n",
    "\n",
    "    # Distribute work among MPI processes\n",
    "    for i, (chunk_start, chunk_end) in enumerate(chunks):\n",
    "        if i % size == rank:  # Each process gets its own months to download\n",
    "            file = download_era5_data(dataset, chunk_start, chunk_end, area, variables, pressure_levels)\n",
    "            files.append(file)\n",
    "\n",
    "    # Gather all downloaded files at root process\n",
    "    all_files = comm.gather(files, root=0)\n",
    "\n",
    "    # Root process merges all files\n",
    "    if rank == 0:\n",
    "        merged_files = [f for sublist in all_files for f in sublist]  # Flatten list\n",
    "        return merge_netcdf_files(merged_files, f\"{dataset}-{start}-{end}.nc\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "def pivot_era5_single(name):\n",
    "    ds = xr.open_dataset(name)\n",
    "    df = ds.to_dataframe()\n",
    "    \n",
    "    df = df.reset_index(level='time', drop=True)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    df.drop(columns=[col for col in ['number', 'expver'] if col in df.columns], inplace=True)\n",
    "    df_reset = df.reset_index()\n",
    "    feature_columns = df_reset.columns[3:]\n",
    "\n",
    "    df_pivot = df_reset.pivot(index=\"valid_time\", columns=[\"latitude\", \"longitude\"], values=feature_columns)\n",
    "    df_pivot.columns = [f\"{lat}-{lon}_{feature}\" for feature, lat, lon in df_pivot.columns]\n",
    "\n",
    "    return df_pivot\n",
    "\n",
    "def pivot_era5_pressure(name):\n",
    "    ds = xr.open_dataset(name)\n",
    "    df = ds.to_dataframe()\n",
    "    \n",
    "    df.drop(columns=[col for col in ['number', 'expver'] if col in df.columns], inplace=True)\n",
    "    df_reset = df.reset_index()\n",
    "    feature_columns = df_reset.columns[4:]\n",
    "\n",
    "    df_pivot = df_reset.pivot(index=\"valid_time\", columns=[\"latitude\", \"longitude\", \"pressure_level\"], values=feature_columns)\n",
    "    df_pivot.columns = [f\"{lat}-{lon}_{pressure}_{feature}\" for feature, lat, lon, pressure in df_pivot.columns]\n",
    "\n",
    "    return df_pivot\n",
    "\n",
    "def merge_era5(name_s, name_p):\n",
    "    df_s = pivot_era5_single(name_s)\n",
    "    df_p = pivot_era5_pressure(name_p)\n",
    "\n",
    "    df = df_s.merge(df_p, on='valid_time', how='outer')\n",
    "    df['valid_time'] = df.index\n",
    "    df = df.reset_index(drop=True) \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc51c57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7885b970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bbf827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping already downloaded file: data/reanalysis-era5-single-levels-2023-01-01-2023-01-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-single-levels-2023-02-01-2023-02-28.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-single-levels-2023-03-01-2023-03-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-single-levels-2023-04-01-2023-04-30.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-single-levels-2023-05-01-2023-05-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-single-levels-2023-06-01-2023-06-30.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-single-levels-2023-07-01-2023-07-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-single-levels-2023-08-01-2023-08-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-single-levels-2023-09-01-2023-09-30.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-single-levels-2023-10-01-2023-10-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-single-levels-2023-11-01-2023-11-30.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-single-levels-2023-12-01-2023-12-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-single-levels-2024-01-01-2024-01-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-single-levels-2024-02-01-2024-02-29.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-single-levels-2024-03-01-2024-03-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-single-levels-2024-04-01-2024-04-30.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-single-levels-2024-05-01-2024-05-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-single-levels-2024-06-01-2024-06-30.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-single-levels-2024-07-01-2024-07-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-single-levels-2024-08-01-2024-08-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-single-levels-2024-09-01-2024-09-30.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-single-levels-2024-10-01-2024-10-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-single-levels-2024-11-01-2024-11-30.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-single-levels-2024-12-01-2024-12-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-single-levels-2025-01-01-2025-01-02.nc\n",
      "Merged file already exists: data/reanalysis-era5-single-levels-2023-01-01-2025-01-02.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-pressure-levels-2023-01-01-2023-01-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-pressure-levels-2023-02-01-2023-02-28.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-pressure-levels-2023-03-01-2023-03-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-pressure-levels-2023-04-01-2023-04-30.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-pressure-levels-2023-05-01-2023-05-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-pressure-levels-2023-06-01-2023-06-30.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-pressure-levels-2023-07-01-2023-07-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-pressure-levels-2023-08-01-2023-08-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-pressure-levels-2023-09-01-2023-09-30.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-pressure-levels-2023-10-01-2023-10-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-pressure-levels-2023-11-01-2023-11-30.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-pressure-levels-2023-12-01-2023-12-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-pressure-levels-2024-01-01-2024-01-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-pressure-levels-2024-02-01-2024-02-29.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-pressure-levels-2024-03-01-2024-03-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-pressure-levels-2024-04-01-2024-04-30.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-pressure-levels-2024-05-01-2024-05-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-pressure-levels-2024-06-01-2024-06-30.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-pressure-levels-2024-07-01-2024-07-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-pressure-levels-2024-08-01-2024-08-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-pressure-levels-2024-09-01-2024-09-30.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-pressure-levels-2024-10-01-2024-10-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-pressure-levels-2024-11-01-2024-11-30.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-pressure-levels-2024-12-01-2024-12-31.nc\n",
      "Skipping already downloaded file: data/reanalysis-era5-pressure-levels-2025-01-01-2025-01-02.nc\n",
      "Merged file already exists: data/reanalysis-era5-pressure-levels-2023-01-01-2025-01-02.nc\n"
     ]
    }
   ],
   "source": [
    "start = '2023-01-01'\n",
    "end = '2025-01-02'\n",
    "area =  [41, -110, 37, -102]\n",
    "\n",
    "name_s = parallel_download_era5(\n",
    "        \"reanalysis-era5-single-levels\", start, end, area,\n",
    "        [\"10m_u_component_of_wind\", \"10m_v_component_of_wind\", \"2m_temperature\", \"mean_sea_level_pressure\"]\n",
    "    )\n",
    "\n",
    "name_p = parallel_download_era5(\n",
    "        \"reanalysis-era5-pressure-levels\", start, end, area,\n",
    "        [\"geopotential\", \"u_component_of_wind\", \"v_component_of_wind\"],\n",
    "        pressure_levels=[\"100\", \"250\", \"450\", \"650\", \"800\", \"900\", \"1000\"]\n",
    "    )\n",
    "\n",
    "if rank == 0:\n",
    "        df_era5 = merge_era5(name_s, name_p)\n",
    "        df_era5['valid_time'] = pd.to_datetime(df_era5['valid_time']).dt.floor('H')\n",
    "\n",
    "        df_bdu = pd.read_csv('bdu_clean.csv')\n",
    "        df_bdu['date'] = pd.to_datetime(df_bdu['date']).dt.floor('H')\n",
    "\n",
    "        df_bdu.drop(columns=['month_name', 'year'], inplace=True)\n",
    "\n",
    "        df = df_bdu.merge(df_era5, left_on='date', right_on='valid_time', how='inner')\n",
    "        df.to_csv(os.path.join(DATA_DIR, 'clean_df.csv'), index=False)\n",
    "\n",
    "        print(f\"Merged dataset saved in {DATA_DIR}/clean_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e3b748",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
